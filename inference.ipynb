{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlfow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#run_id = 'c65b6c792ff240d9a2a415dbca52e4e2'\u001b[39;00m\n\u001b[0;32m     21\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 22\u001b[0m df \u001b[38;5;241m=\u001b[39m pd_read_parquet_from_mlflow(path)\n\u001b[0;32m     23\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m, in \u001b[0;36mpd_read_parquet_from_mlflow\u001b[1;34m(artifact_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a parquet file from an mlflow artifact.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    pd.DataFrame of the loaded artifact\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#local_path = download_artifacts(run_id=run_id, artifact_path=artifact_path)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_parquet(artifact_path)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    668\u001b[0m     path,\n\u001b[0;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    676\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    268\u001b[0m     path,\n\u001b[0;32m    269\u001b[0m     filesystem,\n\u001b[0;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    275\u001b[0m         path_or_handle,\n\u001b[0;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    280\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m    141\u001b[0m         path_or_handle, mode, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m    142\u001b[0m     )\n\u001b[0;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.parquet'"
     ]
    }
   ],
   "source": [
    "# Validate the data\n",
    "#from mlflow.artifacts import download_artifacts\n",
    "import pandas as pd\n",
    "\n",
    "def pd_read_parquet_from_mlflow(\n",
    "    #run_id: str, \n",
    "    artifact_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load a parquet file from an mlflow artifact.\n",
    "\n",
    "    Args:\n",
    "        run_id: the mlflow Run ID\n",
    "        artifact_path: the relative (to the run) path to the parquet file\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame of the loaded artifact\n",
    "    \"\"\"\n",
    "    #local_path = download_artifacts(run_id=run_id, artifact_path=artifact_path)\n",
    "    return pd.read_parquet(artifact_path)\n",
    "\n",
    "#run_id = 'c65b6c792ff240d9a2a415dbca52e4e2'\n",
    "path = 'data.parquet'\n",
    "df = pd_read_parquet_from_mlflow(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ethnicity.value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'age'\n",
    "\n",
    "colums = [col for col in df.columns if col.startswith(name)]\n",
    "df[colums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"major\", \"depressive\", \"disorder\"]\n",
    "\n",
    "# Get the columns whose names contain any of the keywords\n",
    "filtered_columns = [col for col in df.columns if any(keyword in col for keyword in keywords)]\n",
    "\n",
    "# Create a new DataFrame with only those columns\n",
    "df[filtered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.deployment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.deployment.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['phq_q1'].notnull()].groupby('deployment').size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.deployment == 'hr-ascent-1']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot number of user_ids per sex\n",
    "df_sex = df.groupby('sex').user_id.nunique().reset_index().sort_values(by='sex')\n",
    "sns.barplot(data=df_sex, x='sex', y='user_id')\n",
    "plt.title('Number of User IDs per Sex')\n",
    "plt.xlabel('Sex')\n",
    "plt.ylabel('Number of User IDs')\n",
    "plt.show()\n",
    "\n",
    "# Plot number of user_ids per age_bucket\n",
    "df_age_bucket = df.groupby('age_bucket').user_id.nunique().reset_index().sort_values(by='age_bucket')\n",
    "sns.barplot(data=df_age_bucket, x='age_bucket', y='user_id')\n",
    "plt.title('Number of User IDs per Age Bucket')\n",
    "plt.xlabel('Age Bucket')\n",
    "plt.ylabel('Number of User IDs')\n",
    "plt.show()\n",
    "\n",
    "# Plot number of user_ids per ethnicity\n",
    "df['ethnicity'] = df.ethnicity.apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) and len(x) == 1 else 'Multiple' if isinstance(x, (list, np.ndarray)) and len(x) > 1 else 'Not Collected')\n",
    "df_ethnicity = df.groupby('ethnicity').user_id.nunique().reset_index().rename(columns={'ethnicity': 'Ethnicity', 'user_id': 'User Count'}).sort_values(by='User Count', ascending=False).reset_index(drop=True)\n",
    "sns.barplot(data=df_ethnicity, x='Ethnicity', y='User Count')\n",
    "plt.title('Number of User IDs per Ethnicity')\n",
    "plt.xlabel('Ethnicity')\n",
    "plt.ylabel('Number of User IDs')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the number of days in study for each user\n",
    "df['ts'] = pd.to_datetime(df['ts'])\n",
    "df_days_in_study = df.groupby('user_id').agg(\n",
    "    days_in_study=('ts', lambda x: (x.max() - x.min()).days)\n",
    ").reset_index()\n",
    "\n",
    "# Plot the distribution of the number of days in study per user\n",
    "sns.histplot(df_days_in_study[df_days_in_study['days_in_study']>0], bins=15)\n",
    "plt.title('Distribution of Number of Days in Study per User')\n",
    "plt.xlabel('Number of Days in Study')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.phq_q1.notnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.phq_q1.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data (1).parquet'\n",
    "df = pd_read_parquet_from_mlflow(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['phq_q1'].notnull()].groupby('deployment').size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.phq_q1.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.phq_q1.notnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[['user_id', 'ts']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.sleep_duration_m7.notnull()].sleep_duration_m7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sleep_duration_m7.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sleep_duration_m7.notnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('deployment').sleep_duration_m7.apply(lambda x: x.notnull().mean())\n",
    "\n",
    "# The aggregations only output a value if all days are available.\n",
    "# We expect sleep_duration to be missing more often than step count, so compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('deployment').step_count_m7.apply(lambda x: x.notnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('deployment').sex.apply(lambda x: x.notnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('deployment').age_bucket.apply(lambda x: x.notnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('deployment').ethnicity.apply(lambda x: x.notnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.deployment == 'hr-ascent-1']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the df into csv file\n",
    "\n",
    "df.to_csv('hr_ascent_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('deployment').sleep_duration_m7.apply(lambda x: x.notnull().mean())\n",
    "df.groupby('deployment').step_count_m7.apply(lambda x: x.notnull().mean())\n",
    "df.groupby('deployment').sex.apply(lambda x: x.notnull().mean())\n",
    "df.groupby('deployment').age_bucket.apply(lambda x: x.notnull().mean())\n",
    "df.groupby('deployment').ethnicity.apply(lambda x: x.notnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['phq_q8'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['step_count_m7'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = [0.80, 0.85, 0.90, 0.95, 0.98, 0.99, 0.995, 0.998, 0.999, 0.9995]\n",
    "\n",
    "# Calculate the specified percentiles\n",
    "percentile_values = df['step_count_m7'].quantile(percentiles)\n",
    "\n",
    "# Display the results\n",
    "print(\"Percentile values for step_count_m7:\")\n",
    "percentile_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_99 = percentile_values[0.99]\n",
    "outliers_99 = df[df['step_count_m7'] > threshold_99]\n",
    "outliers_99[['phq_q1',\t'phq_q2',\t'phq_q3',\t'phq_q4', 'phq_q5',\t'phq_q6',\t'phq_q7',\t'phq_q8',\t'phq_q9',\t'phq2_total',\t'phq8_total',\t'phq9_total',\t'deployment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate skewness\n",
    "step_count_skewness = df['step_count_m7'].skew()\n",
    "phq_q8_skewness = df['phq_q8'].skew()\n",
    "\n",
    "print(f\"Skewness of 'step_count_m7': {step_count_skewness}\")\n",
    "print(f\"Skewness of 'phq_q8': {phq_q8_skewness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['step_count_m7'].notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.histplot(df['step_count_m7'].dropna(), ax=axes[0], kde=True, color='skyblue')\n",
    "axes[0].set_title(\"Distribution of 'step_count_m7'\")\n",
    "axes[0].set_xlabel('Step Count (m7)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "sns.histplot(df['phq_q8'].dropna(), ax=axes[1], kde=True, color='lightgreen')\n",
    "axes[1].set_title(\"Distribution of 'phq_q8'\")\n",
    "axes[1].set_xlabel('PHQ-8 Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot boxplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.boxplot(y=df['step_count_m7'], ax=axes[0], color='skyblue')\n",
    "axes[0].set_title(\"Boxplot of 'step_count_m7'\")\n",
    "\n",
    "sns.boxplot(y=df['phq_q8'], ax=axes[1], color='lightgreen')\n",
    "axes[1].set_title(\"Boxplot of 'phq_q8'\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.step_count_m7.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation coefficient\n",
    "correlation = df['step_count_m7'].corr(df['phq_q8'])\n",
    "\n",
    "# Print the correlation coefficient\n",
    "print(f\"The correlation between step_count_m7 and phq_q8 is: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson correlation coefficient between the step_count_m7 (step count in the past 7 days) and phq_q8 (score on the 8th question of the Patient Health Questionnaire, likely related to depression or mental health) is approximately 0.053.\n",
    "\n",
    "Positive Correlation: The value is positive (0.053), which indicates a positive correlation. This means that, in general, as step_count_m7 increases, phq_q8 also tends to increase, and vice versa.\n",
    "\n",
    "Weak Correlation: The correlation coefficient is close to 0. The closer the correlation is to 0, the weaker the linear relationship. A value of 0.053 is considered a very weak positive correlation.\n",
    "\n",
    "A correlation of 0.053 suggests that there is a minimal positive linear relationship between step count and the phq_q8 score. Increasing step count has a very, very slight tendency to be associated with a slightly higher score on phq_q8, but the relationship is so weak that it's not practically meaningful on its own. You would not be able to reliably predict someone's phq_q8 score based on their step_count_m7 with this correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "# Create the scatter plot using plotly express\n",
    "fig = px.scatter(df, x='step_count_m7', y='phq_q8',\n",
    "                 title='Scatter Plot of step_count_m7 vs phq_q8',\n",
    "                 labels={'step_count_m7': 'Step Count (Past 7 Days)',\n",
    "                         'phq_q8': 'PHQ-Q8 Score'})  # Adding labels for clarity\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lack of Clear Trend: The most striking feature is the absence of a strong or obvious trend. The points don't seem to cluster around a line or curve. This visually confirms the weak correlation (0.0529) you calculated earlier.\n",
    "\n",
    "The PHQ-Q8 scores appear to be taking on only a few discrete values (0, 1, 2, and 3), which is common for survey or questionnaire data where responses are often on a Likert scale or a limited set of choices. This is important because it means PHQ-Q8 isn't a truly continuous variable in this case, which can affect the interpretation of the Pearson correlation.\n",
    "\n",
    "# Due to the discrete nature of phq_q8 limits the usefulness of the Pearson correlation coefficient. \n",
    "Hence we used some alternative approaches to explore the relationship between step_count_m7 and phq_q8, given that phq_q8 is essentially an ordinal categorical variable:\n",
    "\n",
    "1. Stratified Analysis & Summary Statistics:\n",
    "    Calculate summary statistics (mean, median, standard deviation) of step_count_m7 for each phq_q8 value. This lets us to see if there are any systematic differences in step count across the different PHQ-Q8 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Group by 'phq_q8' and calculate descriptive statistics for 'step_count_m7'\n",
    "grouped_stats = df.groupby('phq_q8')['step_count_m7'].describe()\n",
    "grouped_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. count: This is the number of observations (people) that have that specific phq_q8 score. For example:\n",
    "\n",
    "    1) 907 people have a phq_q8 score of 0.\n",
    "    1) 480 people have a phq_q8 score of 1.\n",
    "    1) 253 people have a phq_q8 score of 2.\n",
    "    1) 170 people have a phq_q8 score of 3.\n",
    "\n",
    "2. mean: This is the average step_count_m7 for people with that phq_q8 score. For example:\n",
    "\n",
    "    1) People with phq_q8 = 0 have an average step_count_m7 of 3001.36.\n",
    "    1) People with phq_q8 = 1 have an average step_count_m7 of 3138.41.\n",
    "    1) People with phq_q8 = 2 have an average step_count_m7 of 3360.74.\n",
    "    1) People with phq_q8 = 3 have an average step_count_m7 of 3407.28.\n",
    "\n",
    "Looking at the \"mean\" values, there's a slight increasing trend in the average step count as the phq_q8 score increases (from 3001.36 to 3407.28). This reinforces the very weak positive correlation you calculated earlier.\n",
    "\n",
    "\n",
    "3. The differences in means are relatively small. The difference between the lowest mean (3001.36 for phq_q8 = 0) and the highest mean (3407.28 for phq_q8 = 3) is only about 400 steps.\n",
    "\n",
    "4. There's substantial overlap in the distributions. The standard deviations are large, meaning there's a lot of variability in step counts within each phq_q8 group. The quartiles (25%, 50%, 75%) also show considerable overlap, indicating that people with different phq_q8 scores can have similar step counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Calculate means\n",
    "mean_steps = df.groupby('phq_q8')['step_count_m7'].mean().reset_index()\n",
    "\n",
    "# Create bar chart\n",
    "fig = px.bar(mean_steps, x='phq_q8', y='step_count_m7',\n",
    "             title='Mean Step Count by PHQ-Q8 Score',\n",
    "             labels={'step_count_m7': 'Mean Step Count (Past 7 Days)',\n",
    "                     'phq_q8': 'PHQ-Q8 Score'},\n",
    "             category_orders={\"phq_q8\": [0, 1, 2, 3]})  # Ensure correct ordering\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Calculate means and standard deviations\n",
    "summary_stats = df.groupby('phq_q8')['step_count_m7'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Create error bar plot\n",
    "fig = go.Figure(data=[go.Bar(\n",
    "    x=summary_stats['phq_q8'],\n",
    "    y=summary_stats['mean'],\n",
    "    error_y=dict(type='data',  # value of error bar given in data coordinates\n",
    "                 array=summary_stats['std'])\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Mean Step Count by PHQ-Q8 Score with Standard Deviation',\n",
    "    xaxis_title='PHQ-Q8 Score',\n",
    "    yaxis_title='Mean Step Count (Past 7 Days)'\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Error Bar Plot\n",
    "\n",
    "What it is: An error bar plot is a way to visualize data that includes both a measure of central tendency (like the mean) and a measure of variability (like the standard deviation or standard error). It's especially helpful when you want to compare the means of different groups while also showing how much the data within each group varies.\n",
    "\n",
    "    1. Components:\n",
    "\n",
    "        1. Bars: Each bar represents the average value (mean) of the step_count_m7 for a specific phq_q8 score. The height of the bar corresponds to the mean.\n",
    "\n",
    "        2. Error Bars: The vertical lines extending above and below each bar are the \"error bars.\" In this case, they represent the standard deviation. The length of the error bar indicates the amount of variability in the data. A longer error bar means there's more spread in the step_count_m7 values for that phq_q8 score.\n",
    "\n",
    "    2. Interpretation of your Error Bar Plot:\n",
    "\n",
    "        1. Means: The tops of the bars are all around the 3000-3500 steps range, indicating that the average step count is roughly similar for all phq_q8 scores. There's a very slight upward trend, as we've discussed, but it's subtle.\n",
    "\n",
    "        2. Variability: This is the most important thing the error bars tell us! The error bars are quite large relative to the differences in the means. This means that the variation in step counts within each phq_q8 group is much greater than the variation between the groups. In other words, people with the same phq_q8 score have a wide range of step counts, and this range overlaps substantially with the range of step counts for people with different phq_q8 scores. This is why a linear model (using statsmodels.formula.api as sm) would be useful here.\n",
    "\n",
    "    3. Conclusion: The error bar plot reinforces the idea that there's no strong relationship between phq_q8 and step_count_m7. While there might be a slight trend toward higher step counts for higher phq_q8 scores, the large variability makes this trend unreliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.box(df, x='phq_q8', y='step_count_m7',\n",
    "             title='Box Plots of Step Count vs PHQ-Q8 Score',\n",
    "             labels={'step_count_m7': 'Step Count (Past 7 Days)',\n",
    "                     'phq_q8': 'PHQ-Q8 Score'},\n",
    "            category_orders={\"phq_q8\": [0, 1, 2, 3]}) # Ensure correct ordering\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def analyze(inf: str, target: str):\n",
    "    # Drop rows with missing values in 'step_count_m7' or 'phq_q8'\n",
    "    df_clean = df.dropna(subset=[inf, target])\n",
    "\n",
    "    # Fit the linear regression model\n",
    "    model = smf.ols(formula=f'{target} ~ {inf}', data=df_clean).fit()\n",
    "\n",
    "    # Display the summary of the model\n",
    "    return model.summary()\n",
    "\n",
    "display(analyze('step_count_m7', 'phq_q8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = df.dropna(subset=['step_count_m7', 'phq_q8'])[['step_count_m7', 'phq_q8']]\n",
    "β0 =  0.7682\n",
    "β1 = 0.00001864\n",
    "df_check['phq_q8_pred'] = β0 + β1 * df_check['step_count_m7']\n",
    "\n",
    "df_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.00001864 * 6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple linear regression:\n",
    "\n",
    "phq_q8 = 𝛽0 + 𝛽1*(step_count_m7) + 𝜀\n",
    "\n",
    "where\n",
    "phq_q8 is your dependent variable (the outcome you want to predict).\n",
    "step_count_m7 is your independent variable (the predictor).\n",
    "\n",
    "#### The goal is to see how changes in step_count_m7 are associated with changes in phq_q8.\n",
    "\n",
    "## Key Regression Output\n",
    "\n",
    "1. Coefficients\n",
    "    1) Intercept (β₀) = 0.7682 \n",
    "        This is the predicted value of phq_q8 when step_count_m7 = 0.\n",
    "    2) step_count_m7 (β₁) = 1.864e-05 (or 0.00001864) \n",
    "        means that for every one-unit increase in step_count_m7, the predicted value of phq_q8 increases by 0.00001864.\n",
    "        This is a very small effect.\n",
    "        \n",
    "Scenario A: A person has step_count_m7 = 5000 \n",
    "\n",
    "Predicted phq_q8 = 0.7682 + (0.00001864 × 5000)\n",
    "= 0.7682 + 0.0932\n",
    "= 0.86\n",
    "\n",
    "Scenario B: A person has step_count_m7 = 6000.\n",
    "\n",
    "Predicted phq_q8 = 0.7682 + (0.00001864 × 6000)\n",
    "= 0.7682 + 0.11184\n",
    "= 0.88004\n",
    "\n",
    "So, increasing steps by 1000 (from 5000 to 6000) corresponds to a predicted increase of about 0.02 points in phq_q8 (0.88 − 0.86 = 0.02).\n",
    "\n",
    "## R-squared (Coefficient of determination):\n",
    "It is a statistical measure that represents the proportion of the variance for a dependent variable (in this case, phq_q8) that's explained by an independent variable (in this case, step_count_m7) in a regression model.\n",
    "\n",
    "\n",
    "Here, we built a simple linear regression model to predict phq_q8 based on step_count_m7.\n",
    "The R-squared value you obtain from this model is 0.003 (0.3%). This indicates that the step_count_m7 variable explains only 0.3% of the variability in the phq_q8 variable. In other words, the majority of the variability in phq_q8 is not explained by step_count_m7. This means that the model does not fit the data well and that other factors are likely influencing phq_q8 more significantly.\n",
    "\n",
    "##### Consequence: \n",
    "The model has limited predictive power, and relying on it for accurate predictions would not be advisable. It suggests that step_count_m7 alone is not a strong predictor of phq_q8.\n",
    "\n",
    "\n",
    "\n",
    "## F-statistic Explained\n",
    "The F-statistic is a measure used in regression analysis to assess the overall significance of the model. Specifically, it tests the hypothesis that all regression coefficients are equal to zero (i.e., the model with no predictors is as good as the model with predictors).\n",
    "\n",
    "Meaning, In regression analysis, the F-test is used to test the null hypothesis that all the regression coefficients (except the intercept) are equal to zero. This means that the independent variables in the model do not contribute to explaining the variability in the dependent variable.\n",
    "\n",
    "1. Null Hypothesis (H0): All coefficients are zero (the model with predictors is no better than the model with no predictors).\n",
    "\n",
    "2. Alternative Hypothesis (H1): At least one coefficient is not zero (the model with predictors is better than the model with no predictors).\n",
    "\n",
    "##### Evaluating the F-statistic Value:\n",
    "1. High F-statistic: Indicates strong evidence against the null hypothesis, suggesting that the model with predictors is significantly better than the model with no predictors.\n",
    "\n",
    "2. Low F-statistic: Indicates weak evidence against the null hypothesis, suggesting that the model with predictors is not significantly better than the model with no predictors.\n",
    "\n",
    "We have a dataset with variables step_count_m7 and phq_q8. You build a regression model to predict phq_q8 based on step_count_m7.\n",
    "\n",
    "1. Null Hypothesis (H0): The coefficient for step_count_m7 is zero.\n",
    "\n",
    "2. Alternative Hypothesis (H1): The coefficient for step_count_m7 is not zero.\n",
    "\n",
    "If the F-test indicates that the null hypothesis can be rejected, it suggests that step_count_m7 significantly contributes to the model and improves the prediction of phq_q8.\n",
    "\n",
    "the F-statistic is 5.075 with a corresponding p-value of 0.024. A p-value of 0.024 is less than the common significance level of 0.05, suggesting that,\n",
    "1) we can reject the null hypothesis \n",
    "2) there is a statistically significant linear relationship between step_count_m7 and phq_q8.\n",
    "\n",
    "However, the strength of that relationship is weak, as indicated by the low R-squared value.\n",
    "\n",
    "## Condition Number \n",
    "The condition number is a measure used to evaluate the sensitivity of a regression model's output to changes in the input data. It provides insight into potential issues related to multicollinearity (when predictor variables are highly correlated) and scaling.\n",
    "\n",
    "Example:\n",
    "Here, in the current datafram (df) have a regression model with several predictor variables, including step_count_m7 and phq_q8. We calculated the condition number for this model and found that it is approximately 6.29e+03 (6,290).\n",
    "\n",
    "### Ideal Value for Condition Number:\n",
    "Low Condition Number (< 30): Indicates that the regression model is stable, and there is little to no multicollinearity or scaling issues. The model's predictions are not overly sensitive to changes in the input data.\n",
    "\n",
    "High Condition Number (> 30): Indicates potential multicollinearity or scaling issues. The model's predictions may be unstable and sensitive to small changes in the input data.\n",
    "\n",
    "### Impact of Low and High Condition Number Values:\n",
    "#### Low Condition Number:\n",
    "Example: Condition number = 10\n",
    "\n",
    "Interpretation: The predictor variables in the regression model are not highly correlated, and the model does not have scaling issues. The predictions are stable and reliable.\n",
    "\n",
    "Consequence: The model is likely to perform well, and the regression coefficients are not unduly influenced by small changes in the input data.\n",
    "\n",
    "#### High Condition Number:\n",
    "\n",
    "Example: Condition number = 6,290\n",
    "\n",
    "Interpretation: The large condition number suggests potential multicollinearity or scaling issues. The predictor variables might be highly correlated, or there might be significant differences in the scale of the variables.\n",
    "\n",
    "Consequence: The model's predictions may be unstable, and the regression coefficients might be unreliable. The model is sensitive to small changes in the input data, which can lead to large changes in the output.\n",
    "\n",
    "\n",
    "# Final Verdict:\n",
    "\n",
    "1. Practical Significance (Effect Size):\n",
    "The slope (1.864×10⁻⁵) is extremely small, meaning that even large differences in step count lead to only very minor changes in the predicted phq_q8 score.\n",
    "2. The R-squared value is 0.003, indicating that only about 0.3% of the variability in phq_q8 is explained by step_count_m7. This tells us that while there is a statistically detectable relationship, it is practically negligible.\n",
    "3. Direction of Effect:\n",
    "There is a positive relationship: as step_count_m7 increases, phq_q8 increases slightly.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Statistically Significant but Weak Relationship:\n",
    "    1) Despite achieving statistical significance (p-value < 0.05), the low R-squared value demonstrates that step_count_m7 is not a strong predictor of phq_q8.\n",
    "    \n",
    "    2) The relationship exists but has minimal practical significance in explaining variations in phq_q8.\n",
    "\n",
    "2. Potential Model Issues:\n",
    "    1) The large condition number suggests that there may be scaling issues affecting the model's stability and the reliability of the coefficients.\n",
    "    2) The dependence on a single predictor limits the explanatory power of the model.\n",
    "\n",
    "3. Implications:\n",
    "    1) For Prediction: Relying on step_count_m7 alone to predict phq_q8 would not yield accurate or reliable results.\n",
    "    2) For Understanding: While there is a slight association, step_count_m7 does not meaningfully explain changes in phq_q8.\n",
    "\n",
    "\n",
    "# Recommendations:\n",
    "\n",
    "1. Explore Additional Variables:\n",
    "    1) Include Other Predictors: Consider adding more variables that may influence phq_q8, such as demographic factors, other health metrics, or psychological assessments.\n",
    "    \n",
    "    2) Feature Selection: Use techniques like correlation analysis or domain expertise to select relevant variables.\n",
    "\n",
    "2. Address Scaling Issues:\n",
    "    1) Standardize Variables: Apply scaling methods (e.g., standardization or normalization) to bring variables to a similar scale.\n",
    "    2) Log Transformation: If step_count_m7 has a wide range or skewed distribution, consider a logarithmic transformation to reduce scaling problems.\n",
    "\n",
    "3. Assess Multicollinearity (if adding more predictors):\n",
    "    1) Variance Inflation Factor (VIF): Calculate VIF for each predictor to detect multicollinearity.\n",
    "    2) Remove or Combine Variables: If multicollinearity is present, remove or combine highly correlated predictors.\n",
    "\n",
    "4. Model Evaluation:\n",
    "\n",
    "    1) Residual Analysis: Examine residuals to check assumptions of linear regression (linearity, homoscedasticity, normality).\n",
    "    2) Alternative Models: Explore other modeling approaches (e.g., multiple regression, polynomial regression, machine learning algorithms) that might capture non-linear relationships.\n",
    "\n",
    "5. Data Quality and Collection:\n",
    "    1) Increase Sample Size: A larger dataset can provide more reliable estimates.\n",
    "    2) Data Integrity: Ensure that the data for step_count_m7 and phq_q8 is accurate and collected consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens looking at the 14 day mean?\n",
    "display(analyze('step_count_m14', 'phq_q8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = 'step_count_m7'\n",
    "target = 'phq_q8'\n",
    "\n",
    "def get_binary_age(age_bucket: str):\n",
    "    if '85' in age_bucket:\n",
    "        return 'old'\n",
    "    ub = int(age_bucket.strip().split('-')[-1])\n",
    "    if ub < 45:\n",
    "        return 'young'\n",
    "    return 'old'\n",
    "\n",
    "df_clean = df.dropna(subset=[inf, target, 'sex', 'age_bucket'])\n",
    "df_clean['sex'] = df_clean.sex.astype(str)\n",
    "df_clean['age_binary'] = df_clean.age_bucket.apply(get_binary_age).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[['step_count_m7', 'sex', 'age_binary', 'phq_q8']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final code \n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def get_binary_age(age_bucket: str):\n",
    "    if '85' in age_bucket:\n",
    "        return 'old'\n",
    "    ub = int(age_bucket.strip().split('-')[-1])\n",
    "    if ub < 45:\n",
    "        return 'young'\n",
    "    return 'old'\n",
    "\n",
    "df['age_binary'] = df.age_bucket.apply(lambda x: None if pd.isnull(x) else get_binary_age(x))\n",
    "\n",
    "def analyze(inf: str, target: str, df: pd.DataFrame):\n",
    "    df_clean = df.dropna(subset=[inf, target, 'sex', 'age_binary'])\n",
    "    # Drop 'Other' because data is so small.\n",
    "    df_clean = df_clean[df_clean.sex != 'Other']\n",
    "    df_clean['sex'] = df_clean.sex.astype(str)\n",
    "    df_clean['age_binary'] = df_clean.age_binary.astype(str)\n",
    "    \n",
    "    # Fit the linear regression model\n",
    "    model = smf.ols(formula=f'{target} ~ {inf} : C(sex) : C(age_binary)', data=df_clean).fit()\n",
    "\n",
    "    # Display the summary of the model\n",
    "    return model.summary()\n",
    "\n",
    "\n",
    "print(analyze('step_count_m7', 'phq_q8', df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explanation of Key Values:\n",
    "\n",
    "    1. Dependent Variable: phq_q8 - This is the variable the model is trying to predict.\n",
    "\n",
    "    2. R-squared: 0.020 - This value represents the proportion of variance in the dependent variable (phq_q8) that is explained by the independent variables in the model. In this case, 0.020 means that only 2.0% of the variance in phq_q8 is explained by step_count_m7, sex, and age_binary and their interactions. This is a very low value, indicating a poor model fit.\n",
    "\n",
    "    3. Adjusted R-squared: 0.018 - The adjusted R-squared is similar to R-squared but penalizes the model for including unnecessary predictors. Since the Adjusted R-squared is very close to the R-squared, it suggests the independent variables included have minor value to the model.\n",
    "\n",
    "    4. F-statistic: 9.097 - This is a test statistic that assesses the overall significance of the model. It tests the null hypothesis that all the coefficients of the independent variables are equal to zero. A higher F-statistic suggests that the model is more significant.\n",
    "\n",
    "    4. Prob (F-statistic): 2.83e-07 (or 0.000000283) - This is the p-value associated with the F-statistic. It represents the probability of observing an F-statistic as large as (or larger than) the one calculated, if the null hypothesis were true (i.e., if all the coefficients were truly zero). Since this value is very small (much less than 0.05), the model is statistically significant overall. This means that at least one of the predictors in the model is significantly related to phq_q8. However, remember that a significant p-value doesn't tell you which predictors are significant or how strong the relationship is. It only tells you that the model as a whole is better than a model with no predictors.\n",
    "\n",
    "    \n",
    "    \n",
    "    4. coef: These are the estimated coefficients for each variable in the model. They represent the change in the dependent variable (phq_q8) for a one-unit change in the independent variable, holding all other variables constant. The coefficients are in scientific notation (e.g., -2.343e-05 means -2.343 x 10^-5).\n",
    "        Intercept: 0.7707\n",
    "\n",
    "        step_count_m7:C(sex)[Female]:C(age_binary)[old]: -2.343e-05, p-value = 0.094. This interaction term is not statistically significant (p > 0.05).\n",
    "\n",
    "        step_count_m7:C(sex)[Male]:C(age_binary)[old]: -4.173e-05, \tp-value =  0.029. This interaction term is statistically significant (p < 0.05).\n",
    "\n",
    "        step_count_m7:C(sex)[Female]:C(age_binary)[young]: 3.683e-05, p-value =  0.000. This interaction term is statistically significant (p < 0.05).\n",
    "\n",
    "        step_count_m7:C(sex)[Male]:C(age_binary)[young]: 3.993e-05, p-value = 0.002. This interaction term is statistically significant (p < 0.05).\n",
    "\n",
    "Cond. No.: 4.45e+03 - This is the condition number, which measures the sensitivity of the model to changes in the data. A high condition number (typically > 30) suggests that there may be multicollinearity (high correlation between independent variables) or other numerical problems that can make the model unstable and difficult to interpret. The note at the bottom of the image confirms this concern.\n",
    "\n",
    "\n",
    "2. Quick \"Before and After\" Scenario\n",
    "\n",
    "Let's create a hypothetical scenario to see how the model predicts phq_q8 changes:\n",
    "\n",
    "Base Case:\n",
    "\n",
    "    1. Female\n",
    "    1. Old (age_binary = \"old\")\n",
    "    1. step_count_m7 = 5000\n",
    "\n",
    "phq_q8 = Intercept + (step_count_m7 * coefficient for Female & Old)\n",
    "\n",
    "phq_q8 = 0.7707 + (5000 * -2.343e-05) = 0.7707 - 0.11715 = 0.65355\n",
    "\n",
    "Scenario 1: Increase Step Count\n",
    "\n",
    "    1. Female\n",
    "    2. Old (age_binary = \"old\")\n",
    "    3. step_count_m7 = 10000 (increased from 5000)\n",
    "\n",
    "phq_q8 = 0.7707 + (10000 * -2.343e-05) = 0.7707 - 0.2343 = 0.5364\n",
    "\n",
    "\n",
    "Base Case:\n",
    "\n",
    "    1. Male\n",
    "    1. Old (age_binary = \"old\")\n",
    "    1. step_count_m7 = 5000\n",
    "\n",
    "phq_q8 = Intercept + (step_count_m7 * coefficient for Male & Old)\n",
    "\n",
    "phq_q8 = 0.7707 + (5000 * -4.173e-05) = 0.7707 -0.20865 = 0.56205\n",
    "\n",
    "Scenario 1: Increase Step Count\n",
    "\n",
    "    1. Male\n",
    "    2. Old (age_binary = \"old\")\n",
    "    3. step_count_m7 = 10000 (increased from 5000)\n",
    "\n",
    "phq_q8 = 0.7707 + (10000 * -4.173e-05) = 0.7707 -0.4173 = 0.3534\n",
    "\n",
    "\n",
    "1. Relationship: The model suggests that the relationship between step_count_m7 and phq_q8 depends on the combination of sex and age. \n",
    "    1. For example, for older females, there appears to be a slight negative relationship (higher step count is associated with slightly lower phq_q8), while for young females, there is a slight positive relationship.\n",
    "\n",
    "\n",
    "## Final Verdict:\n",
    "1. Despite achieving statistical significance (low p-value for the F-statistic), this model is not practically useful. Here's why:\n",
    "\n",
    "2. Very Low R-squared: The model explains only 2% of the variance in phq_q8. This means that 98% of the variation in phq_q8 is due to other factors not included in the model.\n",
    "\n",
    "3. Small Coefficients: The coefficients for the interaction terms are very small, indicating that changes in step count have a minimal impact on the predicted phq_q8 score.\n",
    "\n",
    "4. Multicollinearity: The high condition number suggests that there may be multicollinearity, which can make the model unstable and difficult to interpret. The coefficients may not be reliably estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways:\n",
    "The numbers show that only a very small part (1.8%) of the changes in PHQ-8 scores can be explained by these factors.\n",
    "This means other things (like stress, sleep, or lifestyle) likely have a much bigger impact.\n",
    "\n",
    "1. Older men who are more active tend to have slightly lower depression scores.\n",
    "2. Younger women who are more active tend to have slightly higher depression scores.\n",
    "3. For everyone else, activity time doesn’t seem to make much of a difference.\n",
    "\n",
    "### Activity time is not a strong predictor of depression symptoms in this data.\n",
    "### There might be other important factors affecting mental health that weren’t included in this study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "\n",
    "# Refit the model (important to use the same model specification)\n",
    "model = smf.ols(formula=f'{target} ~ {inf} : C(sex) : C(age_binary)', data=df_clean).fit()\n",
    "\n",
    "# Extract design matrix from the model\n",
    "X = model.model.exog\n",
    "\n",
    "# Calculate VIF for each independent variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data[\"feature\"] = model.model.exog_names  #gets the names of the independent variables in the model.\n",
    "\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows the VIF values for each term in your linear regression model. Remember that VIF measures how much the variance of an estimated regression coefficient increases if your predictors are correlated. A high VIF indicates a high degree of multicollinearity, which can make it difficult to interpret the coefficients and assess the importance of individual predictors.\n",
    "\n",
    "\n",
    "\n",
    "Interpreting VIF Values:\n",
    "\n",
    "    1. VIF = 1: No multicollinearity.\n",
    "    2. 1 < VIF < 5: Moderate multicollinearity.\n",
    "    3. VIF >= 5 or 10: High multicollinearity. This suggests that the coefficient estimates may be unreliable.\n",
    "\n",
    "In our case, all the VIF values (excluding the intercept) are below 5, and most are very close to 1. This indicates that there is very little evidence of multicollinearity in your model. This contradicts the earlier warning based on the high condition number!\n",
    "\n",
    "Why the Discrepancy?\n",
    "\n",
    "The high condition number was a potential warning sign, but VIF provides a more specific and reliable assessment of multicollinearity. There are a few reasons why the condition number might be high even when VIF values are low:\n",
    "\n",
    "Scaling Issues: The condition number is sensitive to the scaling of the variables. If the variables have very different scales (e.g., one variable ranges from 0 to 1, while another ranges from 1000 to 10000), this can inflate the condition number even if the variables are not highly correlated. The scaling issues might be the results of different values from step_count_m7, and the dummy variables.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Based on the VIF values, you do not have a multicollinearity problem in your model. Therefore, you don't need to take any specific actions to address multicollinearity (such as removing variables).\n",
    "\n",
    "## Important Note: \n",
    "Even though you don't have multicollinearity, remember that your model still has a very low R-squared (0.020), meaning it doesn't explain much of the variance in phq_q8. The low R-squared is more concerning than a multicollinearity issue because it indicates that the model is simply not capturing the important factors that influence phq_q8. Therefore, it's important to keep working on improving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_actual_vs_predicted(model, df_clean, target):\n",
    "    # Get the actual and predicted values\n",
    "    df_clean['predicted'] = model.fittedvalues\n",
    "    df_clean['sex_age'] = (df_clean['sex'] + '|' + df_clean['age_binary']).apply(lambda x: x.lower())\n",
    "\n",
    "    # Add jitter to the actual target values\n",
    "    jitter = np.random.normal(0, 0.075, size=len(df_clean[target]))\n",
    "    df_clean[f'{target}_jittered'] = df_clean[target] + jitter\n",
    "\n",
    "    # Generate the scatter plot with hue using Plotly Express\n",
    "    fig = px.scatter(df_clean, \n",
    "                     x=f'{target}_jittered', \n",
    "                     y='predicted', \n",
    "                     color='sex_age', \n",
    "                     title=f'Scatterplot of Actual vs Predicted {target} with Hue by Sex and Age',\n",
    "                     labels={f'{target}_jittered': f'Actual {target} (Jittered)', 'predicted': f'Predicted {target}'},\n",
    "                     opacity=0.5)\n",
    "\n",
    "    fig.update_traces(marker=dict(size=10))  # Set marker size\n",
    "    fig.show()\n",
    "\n",
    "# Example usage\n",
    "plot_actual_vs_predicted(model, df_clean, 'phq_q8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot suggests a poor model performance. There is no clear diagonal pattern. Instead, the predicted phq_q8 values seem to cluster around a narrow range between 0.6 and 1.0, regardless of the actual phq_q8 values. This indicates that the model does not accurately capture the variability in the actual phq_q8 scores.\n",
    "\n",
    "The predicted values are not close to the actual values. The clustering of predicted values within a narrow range indicates that the model is underfitting the data and failing to make accurate predictions.\n",
    "The model attempts to predict the actual values, but it does so very poorly. The limited range of predicted values and the lack of a diagonal pattern indicate that the model is not effective at predicting phq_q8.\n",
    "\n",
    "\n",
    "\n",
    "1. Clustering of Predicted Values: The most prominent feature of the plot is the strong horizontal banding of the predicted values. Almost all data points have predicted phq_q8 values between approximately 0.6 and 1.0. This limited range of predicted values is a clear sign that the model is not sensitive to the actual differences in phq_q8 scores.\n",
    "\n",
    "2. Lack of Diagonal Pattern: In a good regression model, you would expect to see the points clustered around a diagonal line going from the bottom-left to the top-right corner. This would indicate that the predicted values are close to the actual values. However, in this plot, there is no such pattern. The points are scattered randomly within the narrow band of predicted values.\n",
    "\n",
    "3. Overlapping Groups: The different color-coded groups (representing sex and age combinations) are mixed together within the bands, without clear separation. This implies that the model is not effectively distinguishing between these groups in terms of predicting phq_q8.\n",
    "\n",
    "4. Discrete Actual Values: Notice the \"jittered\" actual values are clustered around the whole numbers. This indicates that the original phq_q8 scores are discrete or ordinal (taking only integer values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fittedvalues.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fittedvalues.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze('step_count_m7', 'phq_q8', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define your mappings\n",
    "INF_MAPPING = {\n",
    "    'sleep_start': 'Sleep',\n",
    "    'sleep_end': 'Sleep',\n",
    "    'sleep_duration': 'Sleep',\n",
    "    'chronotype': 'Sleep',\n",
    "    'sleep_routine_index': 'Routine',\n",
    "    'time_at_home': 'Social',\n",
    "    'num_location_clusters': 'Social',\n",
    "    'loc_entropy': 'Social',\n",
    "    'device_use_percent': 'Attention',\n",
    "    'active_time': 'Activity',\n",
    "    'activity_percent': 'Activity',\n",
    "    'step_count': 'Activity',\n",
    "    'walking_rate': 'Activity',\n",
    "    'steps_relative_amplitude': 'Routine',\n",
    "}\n",
    "\n",
    "CATEGORY_QUESTION_MAPPING = {\n",
    "    'Attention': ['phq_q7'],\n",
    "    'Activity': ['phq_q8'],\n",
    "    'Sleep': ['phq_q3'],\n",
    "    'Social': ['phq_q1', 'phq_q6'],\n",
    "    'Routine': ['phq_q3', 'phq_q5'],\n",
    "}\n",
    "\n",
    "TOTALS = ['phq2_total', 'phq8_total']\n",
    "\n",
    "# Create a base output directory in the current working directory\n",
    "output_dir = os.path.join(os.getcwd(), \"outputs\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame as a parquet file in the output directory.\n",
    "# (Make sure that your DataFrame `df` is already defined.)\n",
    "parquet_path = os.path.join(output_dir, 'data_proccessed.parquet')\n",
    "df.to_parquet(parquet_path)\n",
    "print(f\"Saved parquet file to: {parquet_path}\")\n",
    "\n",
    "# Loop through each independent variable and target, run analysis, and log results\n",
    "for inf, category in INF_MAPPING.items():\n",
    "    for target in CATEGORY_QUESTION_MAPPING[category] + TOTALS:\n",
    "        col = f'{inf}_m7'\n",
    "        if col not in df.columns:\n",
    "            print(f'Could not find {col}. Skipping...')\n",
    "            continue\n",
    "\n",
    "        # Run your analysis (assuming your analyze function returns a summary as a string)\n",
    "        m = analyze(col, target, df)\n",
    "\n",
    "        # Create directory structure for logging the results: outputs/ols/{category}/{col}/\n",
    "        log_dir = os.path.join(output_dir, \"ols\", category, col)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        log_file = os.path.join(log_dir, f'{target}.txt')\n",
    "\n",
    "        # Write the analysis summary to a text file\n",
    "        with open(log_file, 'w') as f:\n",
    "            f.write(str(m))\n",
    "        print(f\"Logged results for {col} in category {category} to {log_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_scatterplot(input_col: str, target_col: str, jitter_strength: float = 0.1):\n",
    "    # Clean the data\n",
    "    df_clean = df.dropna(subset=[input_col, target_col, 'sex', 'age_binary'])\n",
    "    df_clean = df_clean[df_clean.sex != 'Other']\n",
    "    df_clean['sex'] = df_clean.sex.astype(str)\n",
    "    df_clean['age_binary'] = df_clean.age_binary.astype(str)\n",
    "\n",
    "    # Create a new column for the combination of sex and age_binary\n",
    "    df_clean['sex_age'] = (df_clean['sex'] + '|' + df_clean['age_binary']).apply(lambda x: x.lower())\n",
    "\n",
    "    # Add jitter to the target column\n",
    "    df_clean[f'{target_col}_jitter'] = df_clean[target_col] + np.random.normal(0, jitter_strength, df_clean.shape[0])\n",
    "\n",
    "    # Create the scatterplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=df_clean, x=input_col, y=f'{target_col}_jitter', hue='sex_age')\n",
    "    plt.title(f'Scatterplot of {input_col} vs {target_col} by Sex and Age Group')\n",
    "    plt.xlabel(input_col.replace('_', ' ').title())\n",
    "    plt.ylabel(target_col.replace('_', ' ').title())\n",
    "    plt.legend(title='Sex and Age Group')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "create_scatterplot('step_count_m7', 'phq_q8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations from the Scatter Plot\n",
    "\n",
    "Horizontal Bands: The data points are arranged in distinct horizontal bands. This is because the 'phq_q8' scores are discrete, taking on only a few integer values (likely 0, 1, 2, and 3).\n",
    "\n",
    "Overlapping Data Points: Within each band, there is significant overlap of the data points, making it difficult to discern any clear patterns or trends. This indicates a weak or non-existent relationship between 'step_count_m7' and 'phq_q8'.\n",
    "\n",
    "Color Distribution: The colors (representing different sex and age groups) appear to be evenly distributed within each band. This suggests that the relationship between 'step_count_m7' and 'phq_q8' is similar across different sex and age groups.\n",
    "\n",
    "Lack of Correlation: There is no clear upward or downward trend in the data. The points are scattered randomly within each band, indicating a lack of correlation between 'step_count_m7' and 'phq_q8'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ethnicity(x):\n",
    "    if not isinstance(x, (list, np.ndarray)):\n",
    "        return None\n",
    "    if len(x) < 2:\n",
    "        return str(x[0])\n",
    "    return 'Multiple'\n",
    "\n",
    "df.ethnicity.apply(clean_ethnicity).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "def pd_read_parquet(artifact_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(artifact_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_binary_age(age_bucket: str):\n",
    "    if '85' in age_bucket:\n",
    "        return 'old'\n",
    "    ub = int(age_bucket.strip().split('-')[-1])\n",
    "    if ub < 45:\n",
    "        return 'young'\n",
    "    return 'old'\n",
    "\n",
    "def clean_ethnicity(x):\n",
    "    if not isinstance(x, (list, np.ndarray)):\n",
    "        return None\n",
    "    if len(x) < 2:\n",
    "        return str(x[0])\n",
    "    return 'Multiple'\n",
    "    \n",
    "def clean(input_col, target_col, df):\n",
    "    df_clean = df.dropna(subset=[input_col, target_col, 'sex', 'age_binary'])\n",
    "    df_clean['sex'] = df_clean.sex.astype(str)\n",
    "    df_clean['age_binary'] = df_clean.age_binary.astype(str)\n",
    "\n",
    "    # Create a new column for the combination of sex and age_binary\n",
    "    df_clean['sex_age'] = (df_clean['sex'] + '|' + df_clean['age_binary']).apply(lambda x: x.lower())\n",
    "    return df_clean\n",
    "\n",
    "def create_scatterplot(input_col: str, target_col: str, df_clean, jitter_strength: float = 0.1):\n",
    "    # Add jitter to the target column\n",
    "    df_clean[f'{target_col}_jitter'] = df_clean[target_col] + np.random.normal(0, jitter_strength, df_clean.shape[0])\n",
    "\n",
    "    # Create the scatterplot\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=df_clean, x=input_col, y=f'{target_col}_jitter', hue='sex_age')\n",
    "    plt.title(f'Scatterplot of {input_col} vs {target_col} by Sex and Age Group')\n",
    "    plt.xlabel(input_col.replace('_', ' ').title())\n",
    "    plt.ylabel(target_col.replace('_', ' ').title())\n",
    "    plt.legend(title='Sex and Age Group')\n",
    "    plt.grid(True)\n",
    "    return fig\n",
    "\n",
    "def analyze(inf: str, target: str, df_clean: pd.DataFrame):\n",
    "    # Fit the linear regression model\n",
    "    model = smf.ols(formula=f'{target} ~ {inf} : C(sex) : C(age_binary)', data=df_clean).fit()\n",
    "\n",
    "    # Display the summary of the model\n",
    "    return model\n",
    "\n",
    "def plot_actual_vs_predicted(model, target, df_clean):\n",
    "    # Get the actual and predicted values\n",
    "    df_clean['predicted'] = model.fittedvalues\n",
    "\n",
    "    # Add jitter to the actual target values\n",
    "    jitter = np.random.normal(0, 0.075, size=len(df_clean[target]))\n",
    "    df_clean[f'{target}_jittered'] = df_clean[target] + jitter\n",
    "\n",
    "    # Generate the scatter plot with hue\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=f'{target}_jittered', y='predicted', hue='sex_age', data=df_clean, alpha=0.5)\n",
    "    plt.xlabel(f'Actual {target} (Jittered)')\n",
    "    plt.ylabel(f'Predicted {target}')\n",
    "    plt.title(f'Scatterplot of Actual vs Predicted {target} with Hue by Sex and Age')\n",
    "    plt.grid(True)\n",
    "    return fig\n",
    "\n",
    "INF_MAPPING = {\n",
    "    'sleep_start': 'Sleep',\n",
    "    'sleep_end': 'Sleep',\n",
    "    'sleep_duration': 'Sleep',\n",
    "    'chronotype': 'Sleep',\n",
    "    \n",
    "    'sleep_routine_index': 'Routine',    \n",
    "    'steps_relative_amplitude': 'Routine',\n",
    "    \n",
    "    'time_at_home': 'Social',\n",
    "    'num_location_clusters': 'Social',\n",
    "    'loc_entropy': 'Social',\n",
    "    \n",
    "    'device_use_percent': 'Attention',\n",
    "    \n",
    "    'active_time': 'Activity',\n",
    "    'activity_percent': 'Activity',\n",
    "    'step_count': 'Activity',\n",
    "    'walking_rate': 'Activity',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "CATEGORY_QUESTION_MAPPING = {\n",
    "    'Attention': ['phq_q7'],\n",
    "    'Activity': ['phq_q8'],\n",
    "    'Sleep': ['phq_q3'],\n",
    "    'Social': ['phq_q1', 'phq_q6'],\n",
    "    'Routine': ['phq_q3', 'phq_q5'],\n",
    "}\n",
    "\n",
    "TOTALS = ['phq2_total', 'phq8_total']\n",
    "\n",
    "DEMO_VARS = ['sex', 'age_bucket', 'ethnicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data (1).parquet'\n",
    "\n",
    "df = pd_read_parquet(path)\n",
    "df = df[df['deployment'] == 'hr-ascent-1']\n",
    "df['sex'] = df.sex.astype(str)\n",
    "df['age_binary'] = df.age_bucket.apply(lambda x: None if pd.isnull(x) else get_binary_age(x)).astype(str)\n",
    "df['ethnicity'] = df.ethnicity.apply(clean_ethnicity).astype(str)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Setup Base Output Directory\n",
    "base_output_dir = os.path.join(os.getcwd(), \"all-deployments\")\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Log Descriptive Statistics\n",
    "desc_stats = df.describe(include='all').to_string()\n",
    "desc_stats_file = os.path.join(base_output_dir, \"descriptive_statistics.txt\")\n",
    "with open(desc_stats_file, 'w') as f:\n",
    "    f.write(desc_stats)\n",
    "print(\"Logged descriptive statistics to:\", desc_stats_file)\n",
    "\n",
    "\n",
    "# Plot and Log Distributions for DEMO_VARS    \n",
    "for var in DEMO_VARS:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    order = None\n",
    "    if var == 'age_bucket':\n",
    "        order = sorted(df[var].dropna().unique(), key=lambda x: int(x.split('-')[0]))\n",
    "    sns.countplot(df, x=var, order=order)\n",
    "    plt.title(f'Distribution of {var}')\n",
    "    plt.xlabel(var.replace('_', ' ').title())\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the figure\n",
    "    dist_dir = os.path.join(base_output_dir, \"distributions\")\n",
    "    os.makedirs(dist_dir, exist_ok=True)\n",
    "    dist_file = os.path.join(dist_dir, f'{var}.png')\n",
    "    plt.savefig(dist_file, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Logged distribution plot for {var} at {dist_file}\")\n",
    "\n",
    "# -----------------------\n",
    "# Plot and Log Correlation Heatmap\n",
    "# -----------------------\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr_matrix = df.corr(numeric_only=True)\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "plt.title('Correlation Heatmap')\n",
    "heatmap_file = os.path.join(base_output_dir, 'correlation_heatmap.png')\n",
    "plt.savefig(heatmap_file, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Logged correlation heatmap at:\", heatmap_file)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# For each variable in INF_MAPPING, run regression and create plots\n",
    "# -----------------------\n",
    "for inf, category in INF_MAPPING.items():\n",
    "    col = f'{inf}_m7'\n",
    "    if col not in df.columns:\n",
    "        print(f'Could not find {col}. Skipping...')\n",
    "        continue\n",
    "\n",
    "    # Create a directory for this variable's outputs\n",
    "    for target in CATEGORY_QUESTION_MAPPING[category] + TOTALS:\n",
    "        # Clean the data for the current predictor (col) and target\n",
    "        df_clean = clean(col, target, df)\n",
    "\n",
    "        # Create and save the scatterplot\n",
    "        fig_scatter = create_scatterplot(col, target, df_clean)\n",
    "        scatter_dir = os.path.join(base_output_dir, \"scatter\", category, col)\n",
    "        os.makedirs(scatter_dir, exist_ok=True)\n",
    "        scatter_file = os.path.join(scatter_dir, f'{target}.png')\n",
    "        fig_scatter.savefig(scatter_file, bbox_inches=\"tight\")\n",
    "        plt.close(fig_scatter)\n",
    "        print(f\"Logged scatterplot for {col} and target {target} at {scatter_file}\")\n",
    "\n",
    "        # Run the regression analysis and save the summary to a text file\n",
    "        m = analyze(col, target, df_clean)\n",
    "        ols_dir = os.path.join(base_output_dir, \"ols\", category, col)\n",
    "        os.makedirs(ols_dir, exist_ok=True)\n",
    "        regression_text_file = os.path.join(ols_dir, f'{target}.txt')\n",
    "        with open(regression_text_file, 'w') as f:\n",
    "            f.write(str(m.summary()))\n",
    "        print(f\"Logged regression results for {col} and target {target} at {regression_text_file}\")\n",
    "\n",
    "        # Create and save the actual vs predicted plot\n",
    "        fig_pred = plot_actual_vs_predicted(m, target, df_clean)\n",
    "        predicted_file = os.path.join(ols_dir, f'{target}_actual_vs_predicted.png')\n",
    "        fig_pred.savefig(predicted_file, bbox_inches=\"tight\")\n",
    "        plt.close(fig_pred)\n",
    "        print(f\"Logged actual vs predicted plot for {col} and target {target} at {predicted_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = 'major_'\n",
    "\n",
    "major_columns = [col for col in df.columns if col.startswith(begin)]\n",
    "\n",
    "df[major_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"major\", \"depressive\", \"disorder\"]\n",
    "\n",
    "# Get the columns whose names contain any of the keywords\n",
    "filtered_columns = [col for col in df.columns if any(keyword in col for keyword in keywords)]\n",
    "\n",
    "# Create a new DataFrame with only those columns\n",
    "df[filtered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# active_time_m7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['active_time_m7'].dropna(),  kde=True, color='skyblue')\n",
    "plt.title(\"Distribution of 'active_time_m7'\")\n",
    "plt.xlabel('active_time_m7 (m7)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['active_time_m7'].dropna(), color='skyblue')\n",
    "plt.title(\"Distribution of 'active_time_m7'\")\n",
    "plt.xlabel('active_time_m7 (m7)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.active_time_m7.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Right Skewness: Both the statistical summary (mean > median) and the distribution plot confirm the data is right-skewed. In the distribution plot, most bars are concentrated toward the left side, creating a long right tail. The long tail is where the data is said to be skewed to. This indicates that while most individuals have lower average daily walking times, a smaller number have significantly higher times.\n",
    "\n",
    "2. Main Peak and Frequency: Based on the histogram, the data has a right skew where the distribution is more concentrated at the lower end of the walking time range. A lot of data has below 2500.\n",
    "\n",
    "1. Most people don't move a lot: The biggest bar is on the left side of the chart, meaning lots of people have low \"active_time_m7\" values. Most move for shorter time of the day.\n",
    "\n",
    "1. Few People Walk Alot: As we go to the right, towards higher \"active_time_m7\" values, the bars get smaller. That means fewer and fewer people have really high average walking times. It gets rare to find someone who walks multiple hours per day, every day.\n",
    "\n",
    "1. Some People Are Sedentary: This chart says that there are a group of people who do not do walk at all.\n",
    "\n",
    "### Real-life Conclusions:\n",
    "\n",
    "1. The \"average\" can be misleading: This graph shows that the average (mean) may not accurately represent the walking habits of most people, because the data is positively skewed. The small number of very active people may skew the average upward. So, it would be best to measure people with smaller \"active_time_m7\".\n",
    "\n",
    "2. There's a wide range of activity levels: The long tail on the right side tells us that there's a huge difference in how much different people move during the day, because there is some people with high \"active_time_m7\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_phq(score):\n",
    "    if 0 <= score <= 4:\n",
    "        return 'Minimal Depression'\n",
    "    elif 5 <= score <= 9:\n",
    "        return 'Mild Depression'\n",
    "    elif 10 <= score <= 14:\n",
    "        return 'Moderate Depression'\n",
    "    elif 15 <= score <= 19:\n",
    "        return 'Moderately Severe Depression'\n",
    "    elif 20 <= score <= 27:\n",
    "        return 'Severe Depression'\n",
    "    else:\n",
    "        return 'Invalid Score'\n",
    "\n",
    "def classify_ethnicity(ethnicity_val):\n",
    "    if ethnicity_val == 'White':\n",
    "        return True\n",
    "   # elif ethnicity_val == 'None':\n",
    "    #    return 'Declined'\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "'''def major_depressive_disorder(score):\n",
    "    if 0 <= score <= 14:\n",
    "        return False\n",
    "    elif 15 <= score <= 27:\n",
    "        return True\n",
    "    else:\n",
    "        return 'Invalid Disorder '''\n",
    "\n",
    "\n",
    "# Apply the classification function to the phq_total column\n",
    "df['ethnicity_white'] = df['ethnicity'].apply(classify_ethnicity)\n",
    "\n",
    "# Apply the classification function to the phq_total column\n",
    "df['phq_category'] = df['phq9_total'].apply(classify_phq)\n",
    "\n",
    "# Apply the classification function to the phq_total column\n",
    "#df['major_depressive_disorder'] = df['phq9_total'].apply(major_depressive_disorder)\n",
    "\n",
    "\n",
    "# Specify the category order for phq_category\n",
    "category_order = ['Minimal Depression', 'Mild Depression', 'Moderate Depression', 'Moderately Severe Depression', 'Severe Depression']\n",
    "df['phq_category'] = pd.Categorical(df['phq_category'], categories=category_order, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  ['active_time_m7', 'age_bucket', 'phq_category', 'ethnicity_white', 'gender', 'phq9_total',] # 'major_depressive_disorder']\n",
    "df_clean= df[data].dropna(subset=data)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "# Example list of age buckets (ensure these exist in your DataFrame)\n",
    "age_buckets = ['16-18', '18-24', '25-34', '35-44', '45-54', '55-64', '65-74', '75-84']\n",
    "\n",
    "# Create a Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# For each age bucket, filter the DataFrame and compute ECDF\n",
    "for bucket in age_buckets:\n",
    "    # Filter the DataFrame for the current age bucket\n",
    "    sub_df = df[df['age_bucket'] == bucket]\n",
    "\n",
    "    # If there are rows in this subgroup, compute and plot the ECDF\n",
    "    if len(sub_df) > 0:\n",
    "        # Drop any NaNs in active_time_m7\n",
    "        values = sub_df['active_time_m7'].dropna()\n",
    "\n",
    "        # Compute the ECDF using statsmodels\n",
    "        ecdf = ECDF(values)\n",
    "        \n",
    "        # ecdf.x are the sorted data points, ecdf.y are the cumulative probabilities\n",
    "        x_vals = ecdf.x\n",
    "        y_vals = ecdf.y\n",
    "        \n",
    "        # Add a line trace to the figure for this age bucket\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_vals,\n",
    "                y=y_vals,\n",
    "                mode='lines',\n",
    "                name=bucket\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Update layout to set titles and legends\n",
    "fig.update_layout(\n",
    "    title='ECDF of active_time_m7 by Age Bucket',\n",
    "    xaxis_title='active_time_m7',\n",
    "    yaxis_title='Proportion (ECDF)',\n",
    "    legend_title='Age Bucket'\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.deployment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['active_time_m7'].notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['active_time_m7'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "# Example DataFrame: df_clean\n",
    "# Must have columns: [\"active_time_m7\", \"age_bucket\", \"phq_category\", \"ethnicity_white\", \"gender\", \"major_depressive_disorder\"]\n",
    "\n",
    "groupings = {\n",
    "    'age_bucket': ['16-18', '18-24', '25-34', '35-44', '45-54', '55-64', '65-74', '75-84'],\n",
    "    'phq_category': ['Minimal Depression', 'Mild Depression', 'Moderate Depression',\n",
    "                     'Moderately Severe Depression', 'Severe Depression'],\n",
    "    'ethnicity_white': [True, False],\n",
    "    'gender': ['Woman', 'Man', 'Transgender', 'Gender Queer Or Nonconforming',],\n",
    "    #'major_depressive_disorder': ['Invalid Disorder', True, False]\n",
    "}\n",
    "\n",
    "for grouping_var, categories in groupings.items():\n",
    "    # Create a new figure for each grouping\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # For each category in this grouping, filter and plot the ECDF of active_time_m7\n",
    "    for cat in categories:\n",
    "        # Filter rows where the grouping_var matches cat\n",
    "        sub_df = df[df[grouping_var] == cat]\n",
    "        if sub_df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Drop any NaN values in active_time_m7\n",
    "        values = sub_df['active_time_m7'].dropna()\n",
    "        if values.empty:\n",
    "            continue\n",
    "        \n",
    "        # Compute ECDF\n",
    "        ecdf = ECDF(values)\n",
    "        x_vals = ecdf.x\n",
    "        y_vals = ecdf.y\n",
    "        \n",
    "        # Add a Scatter trace for this category\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_vals,\n",
    "                y=y_vals,\n",
    "                mode='lines',\n",
    "                name=str(cat)  # This label shows in the legend\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Update layout for this figure\n",
    "    fig.update_layout(\n",
    "        title=f\"ECDF of active_time_m7 by {grouping_var}\",\n",
    "        xaxis_title=\"active_time_m7\",\n",
    "        yaxis_title=\"Proportion (ECDF)\",\n",
    "        legend_title=grouping_var,\n",
    "        height=600,\n",
    "        width=800\n",
    "    )\n",
    "    \n",
    "    # Show (or save) the figure\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.active_time_m7.notna()][['active_time_m7']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# activity_percent_m7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['activity_percent_m7'].dropna(),  kde=True, color='skyblue')\n",
    "plt.title(\"Distribution of 'activity_percent_m7'\")\n",
    "plt.xlabel('activity_percent_m7 (m7)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.activity_percent_m7.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['activity_percent_m7'].dropna(),   color='skyblue')\n",
    "plt.title(\"Distribution of 'activity_percent_m7'\")\n",
    "plt.xlabel('activity_percent_m7 (m7)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.activity_percent_m7.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.active_time_m7.notna()][['activity_percent_m7']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "# Example list of age buckets (ensure these exist in your DataFrame)\n",
    "age_buckets = ['16-18', '18-24', '25-34', '35-44', '45-54', '55-64', '65-74', '75-84']\n",
    "\n",
    "# Create a Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# For each age bucket, filter the DataFrame and compute ECDF\n",
    "for bucket in age_buckets:\n",
    "    # Filter the DataFrame for the current age bucket\n",
    "    sub_df = df[df['age_bucket'] == bucket]\n",
    "\n",
    "    # If there are rows in this subgroup, compute and plot the ECDF\n",
    "    if len(sub_df) > 0:\n",
    "        # Drop any NaNs in activity_percent_m7\n",
    "        values = sub_df['activity_percent_m7'].dropna()\n",
    "\n",
    "        # Compute the ECDF using statsmodels\n",
    "        ecdf = ECDF(values)\n",
    "        \n",
    "        # ecdf.x are the sorted data points, ecdf.y are the cumulative probabilities\n",
    "        x_vals = ecdf.x\n",
    "        y_vals = ecdf.y\n",
    "        \n",
    "        # Add a line trace to the figure for this age bucket\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_vals,\n",
    "                y=y_vals,\n",
    "                mode='lines',\n",
    "                name=bucket\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Update layout to set titles and legends\n",
    "fig.update_layout(\n",
    "    title='ECDF of activity_percent_m7 by Age Bucket',\n",
    "    xaxis_title='activity_percent_m7',\n",
    "    yaxis_title='Proportion (ECDF)',\n",
    "    legend_title='Age Bucket'\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "# Example DataFrame: df_clean\n",
    "# Must have columns: [\"activity_percent_m7\", \"age_bucket\", \"phq_category\", \"ethnicity_white\", \"gender\", \"major_depressive_disorder\"]\n",
    "\n",
    "groupings = {\n",
    "    'age_bucket': ['16-18', '18-24', '25-34', '35-44', '45-54', '55-64', '65-74', '75-84'],\n",
    "    'phq_category': ['Minimal Depression', 'Mild Depression', 'Moderate Depression',\n",
    "                     'Moderately Severe Depression', 'Severe Depression'],\n",
    "    'ethnicity_white': [True, False],\n",
    "    'gender': ['Woman', 'Man', 'Transgender', 'Gender Queer Or Nonconforming', ],\n",
    "    #'major_depressive_disorder': ['Invalid Disorder', True, False]\n",
    "}\n",
    "\n",
    "for grouping_var, categories in groupings.items():\n",
    "    # Create a new figure for each grouping\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # For each category in this grouping, filter and plot the ECDF of activity_percent_m7\n",
    "    for cat in categories:\n",
    "        # Filter rows where the grouping_var matches cat\n",
    "        sub_df = df[df[grouping_var] == cat]\n",
    "        if sub_df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Drop any NaN values in activity_percent_m7\n",
    "        values = sub_df['activity_percent_m7'].dropna()\n",
    "        if values.empty:\n",
    "            continue\n",
    "        \n",
    "        # Compute ECDF\n",
    "        ecdf = ECDF(values)\n",
    "        x_vals = ecdf.x\n",
    "        y_vals = ecdf.y\n",
    "        \n",
    "        # Add a Scatter trace for this category\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_vals,\n",
    "                y=y_vals,\n",
    "                mode='lines',\n",
    "                name=str(cat)  # This label shows in the legend\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Update layout for this figure\n",
    "    fig.update_layout(\n",
    "        title=f\"ECDF of activity_percent_m7 by {grouping_var}\",\n",
    "        xaxis_title=\"activity_percent_m7\",\n",
    "        yaxis_title=\"Proportion (ECDF)\",\n",
    "        legend_title=grouping_var,\n",
    "        height=600,\n",
    "        width=800\n",
    "    )\n",
    "    \n",
    "    # Show (or save) the figure\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step_count_m7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['step_count_m7'].dropna(),  kde=True, color='skyblue')\n",
    "plt.title(\"Distribution of'step_count_m7'\")\n",
    "plt.xlabel('step_count_m7 (m7)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.step_count_m7.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword= 'step'\n",
    "\n",
    "a = [ col for col in df.columns if col.startswith(keyword)]\n",
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "# Example list of age buckets (ensure these exist in your DataFrame)\n",
    "age_buckets = ['16-18', '18-24', '25-34', '35-44', '45-54', '55-64', '65-74', '75-84']\n",
    "\n",
    "# Create a Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# For each age bucket, filter the DataFrame and compute ECDF\n",
    "for bucket in age_buckets:\n",
    "    # Filter the DataFrame for the current age bucket\n",
    "    sub_df = df[df['age_bucket'] == bucket]\n",
    "\n",
    "    # If there are rows in this subgroup, compute and plot the ECDF\n",
    "    if len(sub_df) > 0:\n",
    "        # Drop any NaNs in step_count_m7\n",
    "        values = sub_df['step_count_m7'].dropna()\n",
    "\n",
    "        # Compute the ECDF using statsmodels\n",
    "        ecdf = ECDF(values)\n",
    "        \n",
    "        # ecdf.x are the sorted data points, ecdf.y are the cumulative probabilities\n",
    "        x_vals = ecdf.x\n",
    "        y_vals = ecdf.y\n",
    "        \n",
    "        # Add a line trace to the figure for this age bucket\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_vals,\n",
    "                y=y_vals,\n",
    "                mode='lines',\n",
    "                name=bucket\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Update layout to set titles and legends\n",
    "fig.update_layout(\n",
    "    title='ECDF of step_count_m7 by Age Bucket',\n",
    "    xaxis_title='step_count_m7',\n",
    "    yaxis_title='Proportion (ECDF)',\n",
    "    legend_title='Age Bucket'\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "# Example DataFrame: df_clean\n",
    "# Must have columns: [\"step_count_m7\", \"age_bucket\", \"phq_category\", \"ethnicity_white\", \"gender\", \"major_depressive_disorder\"]\n",
    "\n",
    "groupings = {\n",
    "    'age_bucket': ['16-18', '18-24', '25-34', '35-44', '45-54', '55-64', '65-74', '75-84'],\n",
    "    'phq_category': ['Minimal Depression', 'Mild Depression', 'Moderate Depression',\n",
    "                     'Moderately Severe Depression', 'Severe Depression'],\n",
    "    'ethnicity_white': [True, False],\n",
    "    'gender': ['Woman', 'Man', 'Transgender', 'Gender Queer Or Nonconforming', ],\n",
    "    #'major_depressive_disorder': ['Invalid Disorder', True, False]\n",
    "}\n",
    "\n",
    "for grouping_var, categories in groupings.items():\n",
    "    # Create a new figure for each grouping\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # For each category in this grouping, filter and plot the ECDF of step_count_m7\n",
    "    for cat in categories:\n",
    "        # Filter rows where the grouping_var matches cat\n",
    "        sub_df = df[df[grouping_var] == cat]\n",
    "        if sub_df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Drop any NaN values in step_count_m7\n",
    "        values = sub_df['step_count_m7'].dropna()\n",
    "        if values.empty:\n",
    "            continue\n",
    "        \n",
    "        # Compute ECDF\n",
    "        ecdf = ECDF(values)\n",
    "        x_vals = ecdf.x\n",
    "        y_vals = ecdf.y\n",
    "        \n",
    "        # Add a Scatter trace for this category\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_vals,\n",
    "                y=y_vals,\n",
    "                mode='lines',\n",
    "                name=str(cat)  # This label shows in the legend\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Update layout for this figure\n",
    "    fig.update_layout(\n",
    "        title=f\"ECDF of step_count_m7 by {grouping_var}\",\n",
    "        xaxis_title=\"step_count_m7\",\n",
    "        yaxis_title=\"Proportion (ECDF)\",\n",
    "        legend_title=grouping_var,\n",
    "        height=600,\n",
    "        width=800\n",
    "    )\n",
    "    \n",
    "    # Show (or save) the figure\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# walking_rate_m7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['walking_rate_m7'].dropna(),  kde=True, color='skyblue')\n",
    "plt.title(\"Distribution of 'walking_rate_m7'\")\n",
    "plt.xlabel('walking_rate_m7 (m7)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['walking_rate_m7'].dropna(),   color='skyblue')\n",
    "plt.title(\"Distribution of 'walking_rate_m7'\")\n",
    "plt.xlabel('walking_rate_m7 (m7)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.walking_rate_m7.notna()][['walking_rate_m7']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword= 'wal'\n",
    "\n",
    "a = [ col for col in df.columns if col.startswith(keyword)]\n",
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "# Example DataFrame: df_clean\n",
    "# Must have columns: [\"walking_rate_m7\", \"age_bucket\", \"phq_category\", \"ethnicity_white\", \"gender\", \"major_depressive_disorder\"]\n",
    "\n",
    "groupings = {\n",
    "    'age_bucket': ['16-18', '18-24', '25-34', '35-44', '45-54', '55-64', '65-74', '75-84'],\n",
    "    'phq_category': ['Minimal Depression', 'Mild Depression', 'Moderate Depression',\n",
    "                     'Moderately Severe Depression', 'Severe Depression'],\n",
    "    'ethnicity_white': [True, False],\n",
    "    'gender': ['Woman', 'Man', 'Transgender', 'Gender Queer Or Nonconforming', ],\n",
    "    #'major_depressive_disorder': ['Invalid Disorder', True, False]\n",
    "}\n",
    "\n",
    "for grouping_var, categories in groupings.items():\n",
    "    # Create a new figure for each grouping\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # For each category in this grouping, filter and plot the ECDF of walking_rate_m7\n",
    "    for cat in categories:\n",
    "        # Filter rows where the grouping_var matches cat\n",
    "        sub_df = df[df[grouping_var] == cat]\n",
    "        if sub_df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Drop any NaN values in walking_rate_m7\n",
    "        values = sub_df['walking_rate_m7'].dropna()\n",
    "        if values.empty:\n",
    "            continue\n",
    "        \n",
    "        # Compute ECDF\n",
    "        ecdf = ECDF(values)\n",
    "        x_vals = ecdf.x\n",
    "        y_vals = ecdf.y\n",
    "        \n",
    "        # Add a Scatter trace for this category\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_vals,\n",
    "                y=y_vals,\n",
    "                mode='lines',\n",
    "                name=str(cat)  # This label shows in the legend\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Update layout for this figure\n",
    "    fig.update_layout(\n",
    "        title=f\"ECDF of walking_rate_m7 by {grouping_var}\",\n",
    "        xaxis_title=\"walking_rate_m7\",\n",
    "        yaxis_title=\"Proportion (ECDF)\",\n",
    "        legend_title=grouping_var,\n",
    "        height=600,\n",
    "        width=800\n",
    "    )\n",
    "    \n",
    "    # Show (or save) the figure\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword='device'\n",
    "\n",
    "a= [col for col in df.columns if col.startswith(keyword)]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= df[df.device_use_percent_m7 != 'True'][['user_id', 'device_use_percent_m7']]\n",
    "a.device_use_percent_m7= a.device_use_percent_m7.astype('float')\n",
    "a.device_use_percent_m7.value_counts().sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.device_use_percent_m7.describe()\n",
    "#value_counts().sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>active_time_m7</th>\n",
       "      <th>device_use_percent_m7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>9744.285714</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9180</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      active_time_m7  device_use_percent_m7\n",
       "546              NaN                    1.0\n",
       "6996     9744.285714                    1.0\n",
       "9180             NaN                    1.0"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.device_use_percent_m7) > 0.95][['active_time_m7', 'device_use_percent_m7']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.active_time_m7.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
